{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ad-Rian815/Unza_journal_key_word_classification/blob/main/Automatic_Classification_Of_Key_Words_Associated_with_articles_on_Unza_journals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BHIcKZFccf"
      },
      "source": [
        "# Business Understanding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzXRR53RcVZn"
      },
      "source": [
        "## 1.1 Problem Statement\n",
        "Currently, keywords associated with research articles in UNZA journals are not automatically organized or classified. This makes it difficult for researchers, students, and librarians to quickly identify relevant articles or track research trends. A manual process is time-consuming, inconsistent, and limits the usefulness of the institutional repository. Navigation of journals will be a piece of cake!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T91ZAsmoREBD"
      },
      "source": [
        "## 1.2 Business Objectives\n",
        "The goal of this project is to build a system that can automatically classify keywords from articles into meaningful categories (e.g., *Agriculture, Medicine, Computer Science*).  \n",
        "\n",
        "From a real-world perspective, success means:\n",
        "- Improving searchability and retrieval of research articles.\n",
        "- Helping researchers discover related works faster.\n",
        "- Supporting administrators in analyzing research output trends at UNZA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxUYX_Y1WMRM"
      },
      "source": [
        "##1.3 Data Mining Goal\n",
        "  \n",
        "  The technical approach to achieving these objectives is structured into the following data mining goals:\n",
        "\n",
        "   1.**Classification Model Development:** A classification model will be built to  categorize article keywords into predefined classes.\n",
        "\n",
        "  2.**Text Preprocessing:** The raw text data will be prepared for machine learning using standard preprocessing techniques, including tokenization, stop-word removal, and TF-IDF (Term          Frequency-Inverse Document Frequency) for vectorization.\n",
        "\n",
        "  3.**Algorithm Experimentation:** The performance of several classification algorithms will be evaluated to determine the most effective one. The algorithms to be tested include Naïve          Bayes,   Support Vector Machines (SVM), and Decision Trees.\n",
        "  \n",
        "  To wrap things up, the main goal of this project is to build a machine learning model that can automatically classify article keywords. We've broken the work down into two key           parts. First, we'll focus on data preparation by cleaning the text using tokenization and stop-word removal, and then we'll use TF-IDF to turn everything into numbers for the models     to work with.\n",
        "  \n",
        "  For the second part, we'll experiment with different algorithms like Naïve Bayes, SVM, and Decision Trees. By testing them with metrics like precision and F1-score, then we'll           figure out which one is the most accurate. We're hoping that by following these steps, we can successfully create a model that not only classifies keywords effectively but also          shows we've got a solid grasp of the data mining process.\n",
        "\n",
        "   Summary: The workflow involves two main stages.\n",
        "  First, text preprocessing to clean and transform raw keywords into numerical representations.\n",
        "  Second, experimentation with multiple algorithms to identify the most accurate and robust classifier.\n",
        "\n",
        "    1.3.1 Data Preparation\n",
        "       Before building the model the data will be,\n",
        "       Tokenized into smaller units.\n",
        "       Cleaned by removing stop words and irrelevant terms.\n",
        "       Converted into numerical representations using TF-IDF for input into machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnDULKNcWVKB"
      },
      "source": [
        "## 1.4 Project Success Criteria\n",
        "- The model should achieve at least **80% accuracy** on the test dataset.\n",
        "- The classification results must be **interpretable and consistent** across different domains.\n",
        "- The classification outputs are clear and be easily explained to non-technical stakeholders.\n",
        "- The system should reduce the time required to organize keywords compared to manual methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UajRKSlpWt91"
      },
      "source": [
        "# Data Understanding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV0jOq9SJl-H",
        "outputId": "dc94af95-9aed-4468-fead-2fb19f9d5b2f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0Tp-kH5omu6",
        "outputId": "c925527a-155b-445f-b6a9-a15db012f6c1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import os # Import the os module\n",
        "\n",
        "BASE_URL = \"https://journals.unza.zm\"\n",
        "\n",
        "def get_journal_links():\n",
        "    response = requests.get(BASE_URL)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    links = []\n",
        "    for a in soup.select(\"a\"):\n",
        "        href = a.get(\"href\")\n",
        "        if href and \"index.php\" in href:\n",
        "            links.append(BASE_URL + href if not href.startswith(\"http\") else href)\n",
        "    return list(set(links))\n",
        "\n",
        "def scrape_articles(journal_url, max_pages=3):\n",
        "    articles = []\n",
        "    for page in range(1, max_pages+1):\n",
        "        url = f\"{journal_url}?page={page}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for article in soup.select(\"div.title a\"):\n",
        "            article_url = article.get(\"href\")\n",
        "            article_url = article_url if article_url.startswith(\"http\") else BASE_URL + article_url\n",
        "            article_data = scrape_article_metadata(article_url)\n",
        "            if article_data:\n",
        "                articles.append(article_data)\n",
        "            # polite delay\n",
        "            time.sleep(1)\n",
        "    return articles\n",
        "\n",
        "def _extract_author_names(soup):\n",
        "    # 1) Prefer OJS meta tags (names only)\n",
        "    meta_authors = [m.get(\"content\", \"\").strip()\n",
        "                    for m in soup.select('meta[name=\"citation_author\"]')\n",
        "                    if m.get(\"content\")]\n",
        "    if meta_authors:\n",
        "        # de-dup while preserving order\n",
        "        seen = set()\n",
        "        ordered = []\n",
        "        for a in meta_authors:\n",
        "            if a and a not in seen:\n",
        "                seen.add(a)\n",
        "                ordered.append(a)\n",
        "        return \", \".join(ordered)\n",
        "\n",
        "    # 2) Fall back to elements that specifically hold names\n",
        "    name_selectors = [\n",
        "        \".authors .name\",\n",
        "        \".author .name\",\n",
        "        \"li.authors .name\",\n",
        "        \"div#authors .name\",\n",
        "        \".article-authors .name\",\n",
        "    ]\n",
        "    for sel in name_selectors:\n",
        "        els = [e.get_text(strip=True) for e in soup.select(sel)]\n",
        "        els = [e for e in els if e]\n",
        "        if els:\n",
        "            # remove obvious ORCID ids or emails if any slipped in\n",
        "            cleaned = []\n",
        "            for txt in els:\n",
        "                # strip ORCID URLs/ids and emails in case they appear alongside names\n",
        "\n",
        "                txt = txt.replace(\"ORCID\", \"\")\n",
        "                # very light cleanup without changing the site logic\n",
        "                cleaned.append(txt.strip(\" ,;\"))\n",
        "            # de-dup\n",
        "            seen = set()\n",
        "            ordered = []\n",
        "            for a in cleaned:\n",
        "                if a and a not in seen:\n",
        "                    seen.add(a)\n",
        "                    ordered.append(a)\n",
        "            return \", \".join(ordered)\n",
        "\n",
        "    generic = [e.get_text(strip=True) for e in soup.select(\".authors span, .authors div\")]\n",
        "    if generic:\n",
        "        names = []\n",
        "        for t in generic:\n",
        "            # keep short tokens that look like names (very light heuristic)\n",
        "            if len(t.split()) <= 6 and \"http\" not in t.lower() and \"@\" not in t and \":\" not in t:\n",
        "                names.append(t.strip(\" ,;\"))\n",
        "        # if that yields something believable, join; else return joined generic\n",
        "        if names:\n",
        "            # de-dup\n",
        "            seen = set()\n",
        "            ordered = []\n",
        "            for a in names:\n",
        "                if a and a not in seen:\n",
        "                    seen.add(a)\n",
        "                    ordered.append(a)\n",
        "            return \", \".join(ordered)\n",
        "        return \", \".join(generic)\n",
        "    return None\n",
        "\n",
        "\n",
        "def scrape_article_metadata(article_url):\n",
        "    try:\n",
        "        response = requests.get(article_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch {article_url}: Status {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Debug\n",
        "        print(f\"\\n--- Debugging {article_url} ---\")\n",
        "\n",
        "        # Title\n",
        "        title = None\n",
        "        title_selectors = [\"h1.page_title\", \"h1\", \".page_title\", \".article-title\", \"h1.title\"]\n",
        "        for selector in title_selectors:\n",
        "            element = soup.select_one(selector)\n",
        "            if element:\n",
        "                title = element.get_text().strip()\n",
        "                print(f\"Title found with selector '{selector}': {title[:50]}...\")\n",
        "                break\n",
        "\n",
        "        # Authors: (names only)\n",
        "        authors = _extract_author_names(soup)\n",
        "        if authors:\n",
        "            print(f\"Authors (names only): {authors}\")\n",
        "        else:\n",
        "            print(\"Authors not found with name-specific selectors; see page for structure.\")\n",
        "\n",
        "\n",
        "        # Abstract\n",
        "        abstract = None\n",
        "        abstract_selectors = [\"div.abstract\", \".abstract\", \".article-abstract\", \"div.description\"]\n",
        "        for selector in abstract_selectors:\n",
        "            element = soup.select_one(selector)\n",
        "            if element:\n",
        "                abstract = element.get_text().strip()\n",
        "                break\n",
        "\n",
        "        # Keywords\n",
        "        keywords = None\n",
        "        keyword_selectors = [\"div.keywords\", \".keywords\", \".article-keywords\", \".tags\"]\n",
        "        for selector in keyword_selectors:\n",
        "            element = soup.select_one(selector)\n",
        "            if element:\n",
        "                keywords = element.get_text().strip()\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"authors\": authors,\n",
        "            \"abstract\": abstract,\n",
        "            \"keywords\": keywords,\n",
        "            \"url\": article_url\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {article_url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    journal_links = get_journal_links()\n",
        "    print(f\"Found {len(journal_links)} journals\")\n",
        "\n",
        "    all_articles = []\n",
        "    for journal in journal_links[:2]:\n",
        "        print(f\"\\nScraping {journal} ...\")\n",
        "        articles = scrape_articles(journal, max_pages=1)\n",
        "        all_articles.extend(articles)\n",
        "        if len(all_articles) >= 5:\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(all_articles)\n",
        "\n",
        "    # Define the directory path\n",
        "    save_directory = \"/content/drive/My Drive/misc-unza25-csc4792-project_team9\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)\n",
        "        print(f\"Created directory: {save_directory}\")\n",
        "\n",
        "\n",
        "    df.to_csv(os.path.join(save_directory, \"unza_journals.csv\"), index=False)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- SUMMARY ---\")\n",
        "    print(f\"Total articles scraped: {len(all_articles)}\")\n",
        "    print(f\"Articles with authors: {len([a for a in all_articles if a['authors']])}\")\n",
        "    print(f\"Articles with titles: {len([a for a in all_articles if a['title']])}\")\n",
        "\n",
        "\n",
        "    for i, article in enumerate(all_articles[:3]):\n",
        "        print(f\"\\nArticle {i+1}:\")\n",
        "        for key, value in article.items():\n",
        "            if value:\n",
        "                print(f\"  {key}: {str(value)[:100]}{'...' if len(str(value)) > 100 else ''}\")\n",
        "\n",
        "    print(\"Debug scraping completed. Data saved to unza_journals.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjzkonXOZVv",
        "outputId": "98e66aea-3766-4094-89ef-c337594bf04b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVtzFuYxE_VI",
        "outputId": "f70cbad8-b933-4781-b021-a5acb81a1449",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/My Drive/misc-unza25-csc4792-project_team9/unza_journals.csv\")\n",
        "\n",
        "# Show first 5 rows to confirm load\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpTXV3LMQyY9",
        "outputId": "1731e845-2200-42a8-b9aa-afe80e9935d7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"=== Shape ===\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\n=== Info ===\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n=== Missing Values ===\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n=== First Rows ===\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "BgaDmyTyRAbh",
        "outputId": "a7a1b90f-ec22-408d-fd19-1cc7e6ca5184",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# Split multiple authors into list\n",
        "all_authors = []\n",
        "for a in df['authors'].dropna():\n",
        "    for name in a.split(\",\"):\n",
        "        all_authors.append(name.strip())\n",
        "\n",
        "author_counts = Counter(all_authors).most_common(10)\n",
        "\n",
        "# Plot top 10 authors\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=[c[1] for c in author_counts], y=[c[0] for c in author_counts])\n",
        "plt.title(\"Top 10 Most Frequent Authors\")\n",
        "plt.xlabel(\"Number of Articles\")\n",
        "plt.ylabel(\"Author\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcMR_8RRRDhb"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "### Load dataset\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/My Drive/misc-unza25-csc4792-project_team9/unza_journals.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'unza_journals.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Optionally, exit or handle the error further if the dataframe is essential\n",
        "    exit() # Or some other error handling\n",
        "\n",
        "\n",
        "df[\"abstract_wordcount\"] = df[\"abstract\"].dropna().apply(lambda x: len(str(x).split()))\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"abstract_wordcount\"], bins=20, kde=True)\n",
        "plt.title(\"Distribution of Abstract Lengths (Word Count)\")\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Number of Articles\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMGq3xFpRNDC",
        "outputId": "125389f8-0644-4f0b-f362-4c8eb9e0b6a5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "all_keywords = []\n",
        "for k in df['keywords'].dropna():\n",
        "    for word in k.replace(\"Keywords:\", \"\").replace(\"\\n\",\"\").split(\",\"):\n",
        "        word = word.strip().lower()\n",
        "        if word:\n",
        "            all_keywords.append(word)\n",
        "\n",
        "keyword_counts = Counter(all_keywords).most_common(15)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=[c[1] for c in keyword_counts], y=[c[0] for c in keyword_counts])\n",
        "plt.title(\"Top 15 Keywords Across Articles\")\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Keyword\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp_xNsuMRuKb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Clean keywords column\n",
        "df[\"keywords\"] = (\n",
        "    df[\"keywords\"]\n",
        "    .fillna(\"\")\n",
        "    .str.replace(\"Keywords:\", \"\", regex=False)   # remove the prefix\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)        # collapse whitespace\n",
        "    .str.strip()                                 # trim leading/trailing spaces\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdM9TDH3TySs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Clean keywords column\n",
        "df[\"abstract\"] = (\n",
        "    df[\"abstract\"]\n",
        "    .fillna(\"\")\n",
        "    .str.replace(\"abstract:\", \"\", regex=False)   # remove the prefix\n",
        "    .str.replace(r\"\\s+\", \" \", regex=True)        # collapse whitespace\n",
        "    .str.strip()                                 # trim leading/trailing spaces\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y2w3AnqSKEi",
        "outputId": "d0d74934-63be-4a5a-be63-f6aaa4610447",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def generate_summary(df):\n",
        "    summary = []\n",
        "\n",
        "    # Dataset shape\n",
        "    rows, cols = df.shape\n",
        "    summary.append(f\"The dataset contains **{rows} articles** with **{cols} columns**.\")\n",
        "\n",
        "    # Missing values\n",
        "    missing = df.isnull().sum()\n",
        "    missing_cols = missing[missing > 0]\n",
        "    if not missing_cols.empty:\n",
        "        summary.append(\"Some columns have missing values:\")\n",
        "        for col, val in missing_cols.items():\n",
        "            summary.append(f\"- {col}: {val} missing\")\n",
        "    else:\n",
        "        summary.append(\" There are no missing values in the dataset.\")\n",
        "\n",
        "    # Abstract word count\n",
        "    df[\"abstract_wordcount\"] = df[\"abstract\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
        "    avg_len = df[\"abstract_wordcount\"].mean()\n",
        "    summary.append(f\" Abstracts have an average length of about **{avg_len:.0f} words**.\")\n",
        "\n",
        "    # Top authors\n",
        "    all_authors = []\n",
        "    for a in df['authors'].dropna():\n",
        "        for name in a.split(\",\"):\n",
        "            all_authors.append(name.strip())\n",
        "    author_counts = Counter(all_authors).most_common(5)\n",
        "    if author_counts:\n",
        "        summary.append(\"The most frequent authors are:\")\n",
        "        for name, count in author_counts:\n",
        "            summary.append(f\"- {name}: {count} article(s)\")\n",
        "\n",
        "    # Clean and extract keywords\n",
        "    df[\"keywords\"] = (\n",
        "        df[\"keywords\"]\n",
        "        .fillna(\"\")\n",
        "        .str.replace(\"Keywords:\", \"\", regex=False)\n",
        "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "    all_keywords = []\n",
        "    for k in df['keywords']:\n",
        "        for word in k.split(\",\"):\n",
        "            word = word.strip().lower()\n",
        "            if word:\n",
        "                all_keywords.append(word)\n",
        "    keyword_counts = Counter(all_keywords).most_common(5)\n",
        "    if keyword_counts:\n",
        "        summary.append(\"The most common keywords are:\")\n",
        "        for word, count in keyword_counts:\n",
        "            summary.append(f\"- {word}: {count} occurrence(s)\")\n",
        "\n",
        "    return \"\\n\".join(summary)\n",
        "\n",
        "# === Create summary ===\n",
        "print(generate_summary(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXChDRX3Sg4p"
      },
      "source": [
        "### initial Summary\n",
        "The dataset contains **18 articles** with **6 columns**.\n",
        " There are no missing values in the dataset.\n",
        " Abstracts have an average length of about **275 words**.\n",
        "The most frequent authors are:\n",
        "- Brian Chanda Chiluba: 2 article(s)\n",
        "- Esther Munalula Nkandu: 2 article(s)\n",
        "- Munalula Muyangwa Munalula: 2 article(s)\n",
        "- Kris Kapp: 2 article(s)\n",
        "- Kweleka Mwanza: 1 article(s)\n",
        "The most common keywords are:\n",
        "- covid-19: 6 occurrence(s)\n",
        "- disability: 3 occurrence(s)\n",
        "- adolescent reproductive health: 2 occurrence(s)\n",
        "- anthrax: 1 occurrence(s)\n",
        "- bacillus anthracis: 1 occurrence(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VISFNia0Lxq"
      },
      "source": [
        "# Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7F5km33Cevk"
      },
      "source": [
        "Set up the environment and load the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2b3ktDCgG-",
        "outputId": "2e43e072-9fbe-4f08-bfb5-0872725704df",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/drive/My Drive/misc-unza25-csc4792-project_team9/unza_journals.csv\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(\"\\nColumns:\", df.columns.tolist())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'unza_journals.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Removed exit() here\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # Removed exit() here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf1-U8lkDd_o"
      },
      "source": [
        "\n",
        "Display basic information about the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8odQsguMDk3K",
        "outputId": "58d43137-2db2-4413-85f7-a3dc27db4fae",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOAaWiiID3QY"
      },
      "source": [
        "##3.1 Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8WVByi4-T_s",
        "outputId": "e9922ecc-a4d8-48d9-be9d-e6b3ae78ff89",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "df_clean = df.copy()\n",
        "\n",
        "# Handle missing values - even though initial summary shows no missing, let's verify\n",
        "print(\"Missing values before cleaning:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# Fill any potential missing values in text columns with empty strings\n",
        "text_columns = ['title', 'abstract', 'keywords', 'authors']\n",
        "for col in text_columns:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].fillna('')\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# Clean text data function\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to relevant columns\n",
        "for col in ['title', 'abstract', 'keywords']:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].apply(clean_text)\n",
        "        print(f\"Cleaned {col} column\")\n",
        "\n",
        "print(\"\\nSample of cleaned abstract:\")\n",
        "print(df_clean['abstract'].head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9m-ewupXZV9"
      },
      "source": [
        "##3.2 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYW3auglXqHb",
        "outputId": "d310b7b4-f1c0-4a21-f040-1d950edb3e63",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Creating new features...\")\n",
        "\n",
        "# Abstract features\n",
        "df_clean['abstract_word_count'] = df_clean['abstract'].str.split().str.len()\n",
        "df_clean['abstract_char_count'] = df_clean['abstract'].str.len()\n",
        "df_clean['abstract_avg_word_length'] = np.where(\n",
        "    df_clean['abstract_word_count'] > 0,\n",
        "    df_clean['abstract_char_count'] / df_clean['abstract_word_count'],\n",
        "    0\n",
        ")\n",
        "\n",
        "# Title features\n",
        "df_clean['title_word_count'] = df_clean['title'].str.split().str.len()\n",
        "df_clean['title_char_count'] = df_clean['title'].str.len()\n",
        "\n",
        "# Author & keyword counts\n",
        "df_clean['num_authors'] = df_clean['authors'].fillna('').str.split(',').str.len()\n",
        "df_clean['num_keywords'] = df_clean['keywords'].fillna('').str.split(',').str.len()\n",
        "\n",
        "# Publication year\n",
        "if 'publication_date' in df_clean.columns:\n",
        "    df_clean['publication_year'] = pd.to_datetime(\n",
        "        df_clean['publication_date'], errors='coerce'\n",
        "    ).dt.year\n",
        "\n",
        "# Topic extraction\n",
        "topic_categories = {\n",
        "    'health': ['health', 'medical', 'disease', 'covid', 'pandemic', 'treatment'],\n",
        "    'education': ['education', 'learning', 'teaching', 'school', 'student'],\n",
        "    'technology': ['technology', 'digital', 'computer', 'software', 'internet'],\n",
        "    'social': ['social', 'community', 'society', 'cultural', 'behavior'],\n",
        "    'economic': ['economic', 'finance', 'business', 'market', 'investment'],\n",
        "    'environment': ['environment', 'climate', 'sustainability', 'conservation']\n",
        "}\n",
        "\n",
        "def extract_topics(keywords):\n",
        "    if pd.isna(keywords):\n",
        "        return ['other']\n",
        "    tokens = [kw.strip().lower() for kw in str(keywords).split(',')]\n",
        "    topics = [topic for topic, kw_list in topic_categories.items() if any(t in kw_list for t in tokens)]\n",
        "    return topics if topics else ['other']\n",
        "\n",
        "df_clean['topics'] = df_clean['keywords'].apply(extract_topics)\n",
        "\n",
        "# One-hot encode topics\n",
        "mlb = MultiLabelBinarizer()\n",
        "topic_dummies = pd.DataFrame(mlb.fit_transform(df_clean['topics']),\n",
        "                             columns=[f\"topic_{t}\" for t in mlb.classes_],\n",
        "                             index=df_clean.index)\n",
        "df_clean = pd.concat([df_clean, topic_dummies], axis=1)\n",
        "\n",
        "print(\"New features created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLePa1wlyQGh"
      },
      "source": [
        "## 3.3 Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_columns = [\n",
        "    col for col in df_clean.columns\n",
        "    if col not in ['title', 'abstract', 'keywords', 'authors', 'topics', 'publication_date']\n",
        "]\n",
        "\n",
        "print(\"Final feature columns for modeling:\")\n",
        "print(feature_columns)\n",
        "print(f\"\\nTotal features: {len(feature_columns)}\")\n",
        "# Create final prepared dataset\n",
        "X_prepared = df_clean[feature_columns]\n",
        "\n",
        "print(\"Final prepared dataset shape:\", X_prepared.shape)\n",
        "print(\"\\nFirst 5 rows of prepared data:\")\n",
        "print(X_prepared.head())\n",
        "# Save the prepared data\n",
        "try:\n",
        "    X_prepared.to_csv(\"/content/drive/My Drive/misc-unza25-csc4792-project_team9/unza_journals_prepared.csv\", index=False)\n",
        "    print(\"Prepared data saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving prepared data: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Final Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_columns = [\n",
        "    col for col in df_clean.columns\n",
        "    if col not in ['title', 'abstract', 'keywords', 'authors', 'topics', 'publication_date']\n",
        "]\n",
        "\n",
        "print(\"Final feature columns for modeling:\")\n",
        "print(feature_columns)\n",
        "print(f\"\\nTotal features: {len(feature_columns)}\")\n",
        "# Create final prepared dataset\n",
        "X_prepared = df_clean[feature_columns]\n",
        "\n",
        "print(\"Final prepared dataset shape:\", X_prepared.shape)\n",
        "print(\"\\nFirst 5 rows of prepared data:\")\n",
        "print(X_prepared.head())\n",
        "# Save the prepared data\n",
        "try:\n",
        "    X_prepared.to_csv(\"/content/drive/My Drive/misc-unza25-csc4792-project_team9/unza_journals_prepared.csv\", index=False)\n",
        "    print(\"Prepared data saved successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving prepared data: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rcMR_8RRRDhb"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".NET (C#)",
      "language": "C#",
      "name": ".net-csharp"
    },
    "language_info": {
      "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
      "kernelInfo": {
        "defaultKernelName": "csharp",
        "items": [
          {
            "aliases": [],
            "name": "csharp"
          }
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
